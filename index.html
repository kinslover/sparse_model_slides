<!DOCTYPE html>
<html>
<head>
  <title>Sparse Models</title>
  <meta charset="utf-8">
  <meta name="description" content="Sparse Models">
  <meta name="author" content="Ping Jin">
  <meta name="generator" content="slidify" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/default.css" media="all" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/phone.css" 
    media="only screen and (max-device-width: 480px)" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/slidify.css" >
  <link rel="stylesheet" href="libraries/highlighters/highlight.js/css/tomorrow.css" />
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->
  <script data-main="libraries/frameworks/io2012/js/slides" 
    src="libraries/frameworks/io2012/js/require-1.0.8.min.js">
  </script>
  
    <link rel="stylesheet" href = "assets/css/ribbons.css">
  
</head>
<body style="opacity: 0">
  <slides class="layout-widescreen">
    
    <!-- LOGO SLIDE -->
    <!-- END LOGO SLIDE -->
    

    <!-- TITLE SLIDE -->
    <!-- Should I move this to a Local Layout File? -->
    <slide class="title-slide segue nobackground">
      <hgroup class="auto-fadein">
        <h1>Sparse Models</h1>
        <h2>CMPUT 466/551</h2>
        <p>Ping Jin<br/></p>
      </hgroup>
          </slide>

    <!-- SLIDES -->
      <slide class="" id="slide-1" style="background:;">
  <hgroup>
    <h2>Outline</h2>
  </hgroup>
  <article>
    <ul>
<li><h3>Introduction to Dimension Reduction</h3></li>
<li><h3>Linear Regression and Least Squares (Review)</h3></li>
<li><h3>Subset Selection</h3></li>
<li><h3>Shrinkage Method</h3></li>
<li><h3>Beyond Lasso</h3></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-2" style="background:;">
  <hgroup>
    <h2>Part 1: Introduction to Dimension Reduction</h2>
  </hgroup>
  <article>
    <ul>
<li><b>Introduction to Dimension Reduction</b>

<ul>
<li><b>Difference between feature selection and feature extraction</b></li>
<li><b>Feature Selection</b>

<ul>
<li><b>Wrapper method</b></li>
<li><b>Filter method</b></li>
<li><b>Embedded method</b></li>
</ul></li>
<li><b>Feature Extraction</b>

<ul>
<li><b>PCA, ICA...</b></li>
</ul></li>
</ul></li>
<li>Linear Regression and Least Squares (Review)</li>
<li>Subset Selection</li>
<li>Shrinkage Method</li>
<li>Beyond Lasso</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-3" style="background:;">
  <hgroup>
    <h2>Difference between Feature Selection and Feature Extraction</h2>
  </hgroup>
  <article>
      
<div class='left' style='float:left;width:40%'>
 <h3>Feature Selection</h3>

<ul>
<li>chooses a subset of features from the original feature set</li>
</ul>

<h3>Feature Extraction</h3>

<ul>
<li>transforms the original features into new ones</li>
<li>e.g. projects data from high dimensions to low dimensions</li>
</ul>

<h3>Question:</h3>

<ul>
<li>Why do we need two frameworks for dimension reduction?</li>
</ul>


</div>    
<div class='right' style='float:right;width:56%'>
 <p><center><img src="assets/img/fs.png" alt="fs" title="fs"></center></p>

<p><center><img src="assets/img/fe.png" alt="fe" title="fe"></center></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-4" style="background:;">
  <hgroup>
    <h2>Difference between Feature Selection and Feature Extraction</h2>
  </hgroup>
  <article>
    <h3>Example 1: Prostate Cancer</h3>

<p>The data come from a study by Stamey et al.(1989). In this task, we are trying to identify a subset of features that are useful for prediction of the level of prostate-specific antigen (lpsa). Our available feature set is 
\[\{lcavol, lweight, age, lbph, svi, lcp, gleason, pgg45\}.\]
In this case, we would like to have our result as a subset of the whole set, such as
\[\{lcavol, lweight, age, svi, lcp\},\]
which are important for prediction of lpsa.</p>

<p>Feature selection applies.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-5" style="background:;">
  <hgroup>
    <h2>Difference between Feature Selection and Feature Extraction</h2>
  </hgroup>
  <article>
    <h3>Example 2: classification with fMRI data</h3>

<p>fMRI data are 4D images, with one dimension being the time slot.</p>

<p><center><img src="assets/img/fMRI.png" alt="fMRI" title="fmri"></center></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-6" style="background:;">
  <hgroup>
    <h2>Difference between Feature Selection and Feature Extraction</h2>
  </hgroup>
  <article>
    <h3>Example 2: classification with fMRI data</h3>

<ul>
<li><p>fMRI data are 4D images, with one dimension being the time slot. </p></li>
<li><p>Suppose the dimension of images is \(50 \times 50 \times 50\) for single time point and we have 200 time points</p></li>
<li><p>\(50 \times 50 \times 50 \times 200 = 25,000,000\) dimensions in total! This will cause great computation burdun</p></li>
</ul>

<p>In this task, we are not concerned about importance of particular voxels. Our purpose is to decrease the number of dimensions without losing too much information for further prediction task. </p>

<p>Feature extraction applies better.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-7" style="background:;">
  <hgroup>
    <h2>Feature Selection</h2>
  </hgroup>
  <article>
    <h3>Wrapper Methods</h3>

<ul>
<li>search the space of feature subsets</li>
<li>use the training/validation accuracy of a particular classifier as the measure of utility for a candidate subset</li>
</ul>

<h3>Embedded Methods</h3>

<ul>
<li>exploit the structure of speciﬁc classes of learning models to guide the feature selection process</li>
<li>e.g. LASSO. It is embedded as part of the model construction process</li>
</ul>

<h3>Filter Methods</h3>

<ul>
<li>use some general rules/criterions to measure the feature selection results independent of the classifiers</li>
<li>e.g. mutual information based method</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-8" style="background:;">
  <hgroup>
    <h2>Feature Selection</h2>
  </hgroup>
  <article>
    <h3>Comparison</h3>

<table><thead>
<tr>
<th></th>
<th align="center">Wrapper</th>
<th align="right">Filter</th>
<th align="right">Embedded</th>
</tr>
</thead><tbody>
<tr>
<td>Speed</td>
<td align="center">Low</td>
<td align="right">High</td>
<td align="right">Mid</td>
</tr>
<tr>
<td>Chance of Overfitting</td>
<td align="center">High</td>
<td align="right">Low</td>
<td align="right">Mid</td>
</tr>
<tr>
<td>Classifier-Independent</td>
<td align="center">No</td>
<td align="right">Yes</td>
<td align="right">No</td>
</tr>
</tbody></table>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-9" style="background:;">
  <hgroup>
    <h2>Feature Extraction</h2>
  </hgroup>
  <article>
      
<div class='left' style='float:left;width:40%'>
 <ul>
<li><b>A graphical explanation</b>

<ul>
<li>Each data sample has two features</li>
<li>Prefer the direction with larger variance</li>
<li>Original features are transformed into new ones</li>
</ul></li>
</ul>


</div>    
<div class='right' style='float:right;width:60%'>
 <p><img src="assets/img/pca.png" alt="alt text" title="Principle component analysis"></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-10" style="background:;">
  <hgroup>
    <h2>Part 2: Linear Regression and Least Squares (Review)</h2>
  </hgroup>
  <article>
    <ul>
<li>Introduction to Dimension Reduction</li>
<li><b>Linear Regression and Least Squares (Review)</b>

<ul>
<li><b>Least Square Fit</b></li>
<li><b>Gauss Markov</b></li>
<li><b>Bias-Variance tradeoff</b></li>
<li><b>Problems</b></li>
</ul></li>
<li>Subset Selection</li>
<li>Shrinkage Method</li>
<li>Beyond Lasso</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-11" style="background:;">
  <hgroup>
    <h2>Linear Regression and Least Squares (Review)</h2>
  </hgroup>
  <article>
      
<div class='left' style='float:left;width:58%'>
 <h3>Least Squares Fit</h3>

<p>\[
\begin{equation}
\begin{split}
RSS(\beta) &= (\mathbf{y} - \mathbf{X}\beta)^T(\mathbf{y} - \mathbf{X}\beta)\\
\frac{\partial RSS}{\partial \beta} &= -2 \mathbf{X}^T(\mathbf{y} - \mathbf{X}\beta)\\
\hat{\beta} &= (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}
\end{split}
\end{equation}
\]</p>

<h3>Gauss Markov Theorem</h3>

<p>The least squares estimates of the parameters β have the smallest variance among all linear unbiased estimates.</p>

<h3>Question</h3>

<p>Is unbiased assumption necessary?</p>


</div>    
<div class='right' style='float:right;width:38%'>
 <p><img src="assets/img/lr.png" alt="Linear regression" title="Linear regression"></p>

<p><img src="assets/img/ls.png" alt="Least Squares" title="Least squares"></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-12" style="background:;">
  <hgroup>
    <h2>Part 3: Subset Selection</h2>
  </hgroup>
  <article>
    <ul>
<li>Introduction to Dimension Reduction</li>
<li>Linear Regression and Least Squares (Review)</li>
<li><b>Subset Selection</b>

<ul>
<li><b>Best-subset selection</b></li>
<li><b>Forward stepwise selection</b></li>
<li><b>Forward stagewise selection</b></li>
<li><b>Problems</b></li>
</ul></li>
<li>Shrinkage Method</li>
<li>Beyond Lasso</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-13" style="background:;">
  <hgroup>
    <h2>Linear Regression and Least Squares (Review)</h2>
  </hgroup>
  <article>
    <h3>Bias-Variance tradeoff</h3>

<p>\[
\begin{equation}
\begin{split}
MSE(\tilde{\theta} &= E[(\tilde{\theta} - \theta)^2])\\
&= Var(\tilde{\theta} + [E[\tilde{\theta}] - \theta])
\end{split}
\end{equation}
\]</p>

<p>where \(\theta = \alpha^T \beta\). We can trade some bias for much less variance.</p>

<h3>Problems of Least Squares</h3>

<ul>
<li><b>Prediction accuracy</b>: low bias, but high variance, overfitting noise and sensitive to outlier</li>
<li><b>Interpretation</b>: Sometimes, especially when faced with numerous features, we may want a &quot;big picture&quot; of the model</li>
<li>\((\mathbf{X}^T\mathbf{X})\) may be <b>not invertible</b> and thus no closed form solution</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-14" style="background:;">
  <hgroup>
    <h2>Subset Selection</h2>
  </hgroup>
  <article>
    <h3>Best-subset selection</h3>

<ul>
<li>Best subset regression finds for each \(k \in \{0, 1, 2, . . . , p\}\) the subset of size \(k\) that gives smallest residual sum of squares.</li>
<li>An efficient algorithm, the leaps and bounds procedure (Furnival and Wilson, 1974), makes this feasible for \(p\) as large as 30 or 40.</li>
</ul>

<p><center><img src="assets/img/best_sub.png" alt="best_sub" title="best_sub"></center></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-15" style="background:;">
  <hgroup>
    <h2>Subset Selection</h2>
  </hgroup>
  <article>
    <h3>Forward-stepwise selection</h3>

<p>Instead of searching all possible subsets, we can seek a good path through them.</p>

<p><em>Forward-Stepwise Selection</em> builds a model sequentially, adding one variable at a time. At each step, it</p>

<ul>
<li>identifies the best variable (with the highest correlation with the residual error) to include in the <em>active set</em>
\[\mathbf{x_k} = argmax_{\mathbf{x}_j}(|\mathbf{x}^T_j \mathbf{r}|)\]</li>
<li>then updates the least squares fit to include all the active variables</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-16" style="background:;">
  <hgroup>
    <h2>Subset Selection</h2>
  </hgroup>
  <article>
    <h3>Forward-Stagewise Regression</h3>

  
<div class='left' style='float:left;width:45%'>
 <ul>
<li>Initialize the fit vector \(\mathbf{f} = 0\)</li>
<li>Compute the correlation vector 
\[\mathbf{c} = \mathbf{c}(\mathbf{f}) = \mathbf{X}^T(\mathbf{y} - \mathbf{f})\]</li>
<li>\(k = argmax_{j \in \{1,2,..,p\}} |\mathbf{c}_j|\)</li>
<li>Coefficients and fit vector are updated
\[\mathbf{f} \gets \mathbf{f} + \alpha \cdot sign(\mathbf{c}_j) \mathbf{x}_j\]
\[\beta_j \gets \beta_j + \alpha \cdot sign(\mathbf{c}_j)\] </li>
</ul>


</div>    
<div class='right' style='float:right;width:50%'>
 <p><img src="assets/img/stagewise.png" alt="Stagewise" title="Stagewise"></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-17" style="background:;">
  <hgroup>
    <h2>Subset Selection</h2>
  </hgroup>
  <article>
    <h3>Comparison</h3>

  
<div class='left' style='float:left;width:40%'>
 <ul>
<li>It takes at most \(p\) steps for forward-stepwise selection to get the final fit</li>
<li>Forward stagewise selection is a slow fitting algorithm, at each time step we only update one \(\beta_j\), which can take more than \(p\) steps</li>
<li>Forward stagewise is useful in high dimensional problem</li>
</ul>


</div>    
<div class='right' style='float:right;width:55%'>
 <p><center><img src="assets/img/comp1.png" alt="comp1" title="comp"></center></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-18" style="background:;">
  <hgroup>
    <h2>Subset Selection</h2>
  </hgroup>
  <article>
    <h3>Pros</h3>

<ul>
<li>More interpretable and compact model</li>
</ul>

<h3>Cons</h3>

<ul>
<li>It is a discrete process, and thus has high variance and sensitivity to the change in dataset.</li>
<li>Thus may not be able to lower prediction error</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-19" style="background:;">
  <hgroup>
    <h2>Ridge Regression</h2>
  </hgroup>
  <article>
    <ul>
<li>Introduction to Dimension Reduction</li>
<li>Linear Regression and Least Squares (Review)</li>
<li><b>Shrinkage Method</b>

<ul>
<li><b>Ridge Regression</b>

<ul>
<li><b>Formulations and closed form solution</b></li>
<li><b>Singular value decomposition</b></li>
<li><b>Degree of Freedom</b></li>
</ul></li>
<li>Lasso</li>
</ul></li>
<li>Beyond Lasso</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-20" style="background:;">
  <hgroup>
    <h2>Ridge Regression</h2>
  </hgroup>
  <article>
    <ul>
<li><b>Linear regression with \(l_2\)-regularization</b>

<ul>
<li>Least squares with quadratic constraints
\[
\begin{equation}
\hat{\beta}^{ridge}= argmin_{\beta}\sum_{i=1}^N(y_i - \beta_0 - \sum_{j=1}^p\mathbf{x_{ij}}\beta_j)^2, s.t. \sum_{j = 1}^p \beta_j^2 \leq t
\end{equation}
\]</li>
<li>Its dual form
\[
\hat{\beta}^{ridge} = argmin_{\beta}\{\sum_{i=1}^N(y_i - \beta_0 - \sum_{j=1}^p\mathbf{x_{ij}}\beta_j)^2 + \lambda \sum_{j = 1}^p\beta_j^2\}
\]</li>
<li>The \(l_2\)-regularization can be viewed as a Gaussian prior on the coefficients, and our estimates are the posterior means</li>
</ul></li>
<li><b>Solution</b>
\[
\begin{equation}
\begin{split}
&RSS(\lambda) = (\mathbf{y} - \mathbf{X}\beta)^T(\mathbf{y} - \mathbf{X}\beta) + \lambda \beta^T\beta\\
&\hat{\beta}^{ridge} = (\mathbf{X}^T\mathbf{X} + \lambda \mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}
\end{split}
\end{equation}
\]<br></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-21" style="background:;">
  <hgroup>
    <h2>Ridge Regression</h2>
  </hgroup>
  <article>
    <h3>Singular Value Decomposition (SVD)</h3>

<ul>
<li><b>The SVD of</b> \(\mathbf{X}\):
\[\mathbf{X} = \mathbf{UDV}^T\]

<ul>
<li>\(\mathbf{U}\): \(N \times p\) <b>orthogonal</b> matrix with columns spanning the column space of \(\mathbf{X}\)</li>
<li>\(\mathbf{V}\): \(p \times p\) <b>orthogonal</b> matrix with columns spanning the row space of \(\mathbf{X}\)<br></li>
<li>\(\mathbf{D}\): \(p \times p\) <b>diagonal</b> matrix with diagonal entries \(d_1 \geq d_2 \geq ... \geq d_p \geq 0\) being the singular values of \(\mathbf{X}\)</li>
</ul></li>
<li><b>For least squares</b>
\[
\begin{equation}
\begin{split}
\mathbf{X}\hat{\beta}^{ls} &= \mathbf{X(X^TX)^{-1}X^Ty}\\
&=\mathbf{UU^Ty}
\end{split}
\end{equation}
\]</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-22" style="background:;">
  <hgroup>
    <h2>Ridge Regression</h2>
  </hgroup>
  <article>
    <h3>Singular Value Decomposition (SVD)</h3>

<ul>
<li><b>For ridge regression</b>
\[
\begin{equation}
\begin{split}
\mathbf{X}\hat{\beta}^{ridge} &= \mathbf{X(X^TX + \lambda I)^{-1}X^Ty}\\
&=\sum_{j=1}^p\mathbf{u}_j\frac{d_j^2}{d_j^2 + \lambda} \mathbf{u}_j^T\mathbf{y}
\end{split}
\end{equation}
\]

<ul>
<li>Compared with the solution of least square, we have an additional shrinkage term, the smaller d is and the larger λ is, the more shrinkage we have. </li>
</ul></li>
<li>The SVD of the centered matrix \(X\) is another way of expressing the principal components of the variables in \(X\). </li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-23" style="background:;">
  <hgroup>
    <h2>Ridge Regression</h2>
  </hgroup>
  <article>
    <h3>Singular Value Decomposition (SVD)</h3>

<p><center><img src="assets/img/pc.png" alt="PC" title="pc"></center></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-24" style="background:;">
  <hgroup>
    <h2>Ridge Regression</h2>
  </hgroup>
  <article>
    <h3>Degree of Freedom</h3>

  
<div class='left' style='float:left;width:40%'>
 <ul>
<li>In statistics, the number of degrees of freedom is the number of values in the final calculation of a statistic that are free to vary.</li>
<li>Computation
\[
\begin{equation}
\begin{split}
d(\lambda) &= tr[\mathbf{X(X^TX + \lambda I)^{-1}X^T}]\\
&=tr[\mathbf{H_{\lambda}}]\\
&=\sum_{j=1}^p \frac{d_j^2}{d_j^2 + \lambda} 
\end{split}
\end{equation}
\]</li>
<li>The smaller \(d\) is and the larger \(\lambda\) is, the less degree of freedom we have</li>
</ul>


</div>    
<div class='right' style='float:right;width:55%'>
 <p><center><img src="assets/img/df.png" alt="df" title="df"></center></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-25" style="background:;">
  <hgroup>
    <h2>LASSO</h2>
  </hgroup>
  <article>
    <ul>
<li>Introduction to Dimension Reduction</li>
<li>Linear Regression and Least Squares (Review)</li>
<li><b>Shrinkage Method</b>

<ul>
<li>Ridge Regression</li>
<li><b>Lasso</b>

<ul>
<li><b>Formulations</b></li>
<li><b>Comparisons with ridge regression and subset selection</b></li>
<li><b>Quadratic Programming</b></li>
<li><b>Least Angle Regression</b></li>
<li><b>Viewed as approximation for \(l_0\)-regularization</b></li>
</ul></li>
</ul></li>
<li>Beyond Lasso</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-26" style="background:;">
  <hgroup>
    <h2>LASSO</h2>
  </hgroup>
  <article>
    <h3>Linear regression with \(l_1\)-regularization</h3>

<ul>
<li><p><b>Problems with \(l_2\)-regularization</b></p>

<ul>
<li>Interpretability and compactness: Though coefficients are shrinked, but not to zero.</li>
<li>Least squares with constraints
\[
\begin{equation}
\hat{\beta}^{ridge}= argmin_{\beta}\sum_{i=1}^N(y_i - \beta_0 - \sum_{j=1}^p\mathbf{x_{ij}}\beta_j)^2, s.t. \sum_{j = 1}^p |\beta_j| \leq t
\end{equation}
\]</li>
<li>Its dual form
\[
\hat{\beta}^{ridge} = argmin_{\beta}\{\sum_{i=1}^N(y_i - \beta_0 - \sum_{j=1}^p\mathbf{x_{ij}}\beta_j)^2 + \lambda \sum_{j = 1}^p|\beta_j|\}
\]</li>
<li>The \(1_1\)-regularization can be viewed as a Laplace prior on the coefficients</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-27" style="background:;">
  <hgroup>
    <h2>LASSO</h2>
  </hgroup>
  <article>
    <p><center><img src="assets/img/lasso.png" alt="lasso" title="lasso"></center></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-28" style="background:;">
  <hgroup>
    <h2>LASSO</h2>
  </hgroup>
  <article>
    <ul>
<li>Introduction to Dimension Reduction</li>
<li>Linear Regression and Least Squares (Review)</li>
<li><b>Shrinkage Method</b>

<ul>
<li>Ridge Regression</li>
<li><b>Lasso</b>

<ul>
<li>Formulations</li>
<li><b>Comparisons with ridge regression and subset selection</b>

<ul>
<li><b>Orthonormal inputs</b></li>
<li><b>Non-orthonormal inputs</b></li>
</ul></li>
<li>Quadratic Programming</li>
<li>Least Angle Regression</li>
<li>Viewed as approximation for \(l_0\)-regularization</li>
</ul></li>
</ul></li>
<li>Beyond Lasso</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-29" style="background:;">
  <hgroup>
    <h2>LASSO</h2>
  </hgroup>
  <article>
    <h3>Comparison</h3>

<ul>
<li><b>Orthonormal Input \(\mathbf{X}\)</b>

<ul>
<li><b>Best subset</b>: [Hard thresholding] Only keep the top \(M\) largest coefficeints of \(\hat{\beta}^{ls}\)</li>
<li><b>Ridge</b>: [Pure shrinkage] does proportional shrinkage of \(\hat{\beta}^{ls}\)</li>
<li><b>Lasso</b>: [Soft thresholding] translates each coefficient of \(\hat{\beta}^{ls}\) by \(\lambda\), truncating at 0 </li>
</ul></li>
</ul>

<p><center><img src="assets/img/comp2.png" alt="comp2" title="comp2"></center></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-30" style="background:;">
  <hgroup>
    <h2>LASSO</h2>
  </hgroup>
  <article>
    <h3>Comparison</h3>

<ul>
<li><b>Non-orthonormal Input \(\mathbf{X}\)</b></li>
</ul>

<p><center><img src="assets/img/comp3.png" alt="comp3" title="comp3"></center></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-31" style="background:;">
  <hgroup>
    <h2>LASSO</h2>
  </hgroup>
  <article>
    <h3>Other unit circles for different \(p\)-norms</h3>

<p><center><img src="assets/img/unit_circle.png" alt="uc" title="uc"></center></p>

<table><thead>
<tr>
<th></th>
<th>Convex</th>
<th>Smooth</th>
<th>Sparse</th>
</tr>
</thead><tbody>
<tr>
<td>\(q<1\)</td>
<td>No</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>\(q>1\)</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>\(q = 1\)</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
</tr>
</tbody></table>

<p>Here \(q = 0\) is the pure variable selection procedure, as it is counting the <b>number of non-zero coefficients</b>.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-32" style="background:;">
  <hgroup>
    <h2>LASSO</h2>
  </hgroup>
  <article>
    <h3>Regularizations as priors</h3>

<p>\(|\beta_j|^q\) can be viewed as the log-prior density for \(\beta_j\), these three methods are bayes estimates with different priors</p>

<ul>
<li><b>Subset selection</b>: corresponds to \(q = 0\)</li>
<li><b>LASSO</b>: corresponds to \(q = 1\), Laplace prior, \(density = (\frac{1}{\tau})exp(\frac{-|\beta|}{\tau}), \tau = 1/\lambda\)</li>
<li><b>Ridge regression</b>: corresponds to \(q = 2\), Gaussian Prior</li>
</ul>

  
<div class='left' style='float:left;width:48%'>
 <p><center><img src="assets/img/laplace.png" alt="laplace" title="laplace"></center></p>


</div>    
<div class='right' style='float:right;width:48%'>
 <p><center><img src="assets/img/gauss.png" alt="gauss" title="gauss"></center></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-33" style="background:;">
  <hgroup>
    <h2>LASSO</h2>
  </hgroup>
  <article>
    <ul>
<li>Introduction to Dimension Reduction</li>
<li>Linear Regression and Least Squares (Review)</li>
<li><b>Shrinkage Method</b>

<ul>
<li>Ridge Regression</li>
<li><b>Lasso</b>

<ul>
<li>Formulations</li>
<li>Comparisons with ridge regression and subset selection</li>
<li><b>Quadratic Programming</b></li>
<li>Least Angle Regression</li>
<li>Viewed as approximation for \(l_0\)-regularization</li>
</ul></li>
</ul></li>
<li>Beyond Lasso</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-34" style="background:;">
  <hgroup>
    <h2>LASSO</h2>
  </hgroup>
  <article>
    <h3>Quadratic Programming</h3>

<ul>
<li>Formulation
\[
min_{\beta}\{ \frac{1}{2}(\mathbf{X}\beta - \mathbf{y})^T (\mathbf{X}\beta - \mathbf{y}) + \lambda \|\beta\|_1\}
\]
is equivalent to 
\[
min_{w, \xi}\{ \frac{1}{2}(\mathbf{X}\beta - \mathbf{y})^T (\mathbf{X}\beta - \mathbf{y}) + \lambda \mathbf{1}^T\xi\}
\]</li>
</ul>

<p>\[
\begin{equation}
\begin{split}
s.t. &\beta_j \leq \xi_j\\
&\beta_j \geq -\xi_j
\end{split}
\end{equation}
\]</p>

<ul>
<li>Note that QP can only solve LASSO with a fixed \(\lambda\)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-35" style="background:;">
  <hgroup>
    <h2>LASSO</h2>
  </hgroup>
  <article>
    <ul>
<li>Introduction to Dimension Reduction</li>
<li>Linear Regression and Least Squares (Review)</li>
<li><b>Shrinkage Method</b>

<ul>
<li>Ridge Regression</li>
<li><b>Lasso</b>

<ul>
<li>Formulations</li>
<li>Comparisons with ridge regression and subset selection</li>
<li>Quadratic Programming</li>
<li><b>Least Angle Regression</b></li>
<li>Viewed as approximation for \(l_0\)-regularization</li>
</ul></li>
</ul></li>
<li>Beyond Lasso</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-36" style="background:;">
  <hgroup>
    <h2>Least Angle Regression</h2>
  </hgroup>
  <article>
    <h3>Notations</h3>

<ul>
<li>\(\mathcal{A}_k\): <i>active set</i>, the set of features we already included in the model at time step \(k\)</li>
<li>\(\beta_{\mathcal{A}_k}\): coefficients vector at the beginning of time step \(k\)</li>
<li>\(\beta_{\mathcal{A}_k}(\alpha)\): coefficients vector in time step \(k\) w.r.t. \(\alpha\), </li>
<li>\(\mathbf{f}_k\): the fit vector at the beginning of time step \(k\), \(\mathbf{f}_0 = 0\)</li>
<li>\(\mathbf{f}_k(\alpha)\): the fit vector in time step \(k\) w.r.t. \(\alpha\)</li>
<li>\(\mathbf{r}_k\): residual vector at the beginning of time step \(k\), \(\mathbf{r}_0 = \mathbf{y} - \bar{\mathbf{y}}\)</li>
<li>\(\mathbf{r}_k(\alpha)\): residual vector in time step \(k\), w.r.t. \(\alpha\)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-37" style="background:;">
  <hgroup>
    <h2>LAR Algorithm</h2>
  </hgroup>
  <article>
    <ul>
<li>Initialization: 

<ul>
<li>Standardized all predictors s.t. \(\bar{\mathbf{x}_j} = 0, \mathbf{x}_j^T\mathbf{x}_j = 1\); \(\mathbf{r}_0 = \mathbf{y} - \bar{\mathbf{y}}\); \(\beta = \mathbf{0}\); \(\mathbf{f}_0 = \mathbf{0}\); \(\mathcal{A}_k = \emptyset\)</li>
<li>\(\mathbf{x}_k = argmax_{\mathbf{x}_j} |\mathbf{x}_j^T \mathbf{r}_0|\), \(\mathcal{A}_1 = \{\mathbf{x}_k\}\)</li>
</ul></li>
<li>Main

<ul>
<li>While termination_cond != true

<ul>
<li>\(\mathbf{r}_k = \mathbf{y} - \mathbf{X}_{\mathcal{A}_k} \beta_{\mathcal{A}_k}\), \(\mathbf{f}_k = \mathbf{X}_{\mathcal{A}_k} \beta_{\mathcal{A}_k}\)</li>
<li>Search \(\alpha\)

<ul>
<li>\(\beta_{\mathcal{A}_k}(\alpha) = \mathcal{A}_k + \alpha \cdot \delta_k\), where \(\delta_k = \mathbf{(X^T_{\mathcal{A}_k} X_{\mathcal{A}_k})^{-1} X^T_{\mathcal{A}_k}r_k}\)</li>
<li>Concurrently, \(\mathbf{f}_k(\alpha) = \mathbf{f}_k + \alpha \cdot \mathbf{u}_k\), where \(\mathbf{u}_k = \mathbf{X}_{\mathcal{A}_k} \delta_k\)</li>
</ul></li>
<li>Until \(|\mathbf{X}_{\mathcal{A}_k} \mathbf{r}_k(\alpha)| = max_{\mathbf{x}_j \in \bar{\mathcal{A}_k}} |\mathbf{x}_j^T \mathbf{r}_k(\alpha)|\)</li>
<li>\(\mathbf{x}_k = argmax_{\mathbf{x}_j \in \bar{\mathcal{A}_k}} |\mathbf{x}_j \mathbf{r}_k(\alpha)|\) </li>
<li>\(\mathcal{A}_{k+1} = \mathcal{A}_{k} \cup \{\mathbf{x}_k\}\)</li>
</ul></li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-38" style="background:;">
  <hgroup>
    <h2>LAR</h2>
  </hgroup>
  <article>
    <h3>Comments</h3>

<ol>
<li><b>Why called Least Angle</b>: the direction \(\mathbf{u}_k = \mathbf{X}_{\mathcal{A}_k} \delta_k\) that our fit \(\mathbf{f}_k(\alpha)\) increases actually has the same angle with any \(\mathbf{x}_j \in \mathcal{A}_k\).</li>
<li>Note that the left-hand side of the termination condition for searching is a vector, while the right-hand side is a single value. 
\[|\mathbf{X}_{\mathcal{A}_k} \mathbf{r}_k(\alpha)| = max_{\mathbf{x}_j \in \bar{\mathcal{A}_k}} |\mathbf{x}_j^T \mathbf{r}_k(\alpha)|\]
This actually comes from the fact that the absolute values of correlations of \(\mathbf{x}_j \in \mathcal{A}_k, \forall j\) with the residual error are tied and decrease at the same rate.</li>
<li>The procedure of searching is approaching the least-squares coefficients of fitting \(\mathbf{y}\) on \(\mathcal{A}_k\)</li>
<li>LAR solves the subset selection problem for all \(t, s.t. \|\beta\| \leq t\)</li>
<li>Actually, \(\alpha\) can be computed instead of searching</li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-39" style="background:;">
  <hgroup>
    <h2>LAR</h2>
  </hgroup>
  <article>
    <h3>Example</h3>

<p><center><img src="assets/img/lars.png" alt="Lars" title="lars"></center></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-40" style="background:;">
  <hgroup>
    <h2>LAR</h2>
  </hgroup>
  <article>
    <h3>Result compared with LASSO</h3>

  
<div class='left' style='float:left;width:28%'>
 <h4>Observations:</h4>

<p>When the blue line coefficient cross zero, LAR and LASSO become different.</p>


</div>    
<div class='right' style='float:right;width:72%'>
 <p><center><img src="assets/img/comp4.png" alt="comp4" title="comp4"></center></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-41" style="background:;">
  <hgroup>
    <h2>LAR</h2>
  </hgroup>
  <article>
    <h3>Modification for LASSO</h3>

<p>During the searching procedure, if a non-zero coefficient hits zero, drop this variable from \(\mathcal{A}_k\), and recompute the direction \(\delta_k\)</p>

<h3>Some heuristic analysis</h3>

<ul>
<li>At a certain time point, we know that all \(\mathbf{x}_j \in \mathcal{A}\) share the same absolute values of correlations with the residual error. That is
\[\mathbf{x}_j^T(\mathbf{y} - \mathbf{X}\beta) = \gamma \cdot s_j, \forall \mathbf{x}_j \ \in \mathcal{A}\]
where \(s_j \in \{-1,1\}\) indicates the sign of the left hand inner product and \(\gamma\) is the common value. We also know that \(|\mathbf{x_j}(\mathbf{y} - \mathbf{X}\beta)| \leq \gamma, \forall \mathbf{x}_j \not\in \mathcal{A}\)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-42" style="background:;">
  <hgroup>
    <h2>LAR</h2>
  </hgroup>
  <article>
    <h3>Some heuristic analysis</h3>

<ul>
<li><p>At a certain time point, we know that all \(\mathbf{x}_j \in \mathcal{A}\) share the same absolute values of correlations with the residual error. That is
\[\mathbf{x}_j^T(\mathbf{y} - \mathbf{X}\beta) = \gamma \cdot s_j, \forall \mathbf{x}_j \ \in \mathcal{A}\]
where \(s_j \in \{-1,1\}\) indicates the sign of the left hand inner product and \(\gamma\) is the common value. We also know that \(|\mathbf{x_j}(\mathbf{y} - \mathbf{X}\beta)| \leq \gamma, \forall \mathbf{x}_j \not\in \mathcal{A}\)</p></li>
<li><p>Now consider about LASSO for a fixed given \(\lambda\). Let \(\mathcal{B}\) with non-zero coefficients, then we differentiate the objective function w.r.t. these non zero coefficients and set the gradient to zero
\[\mathbf{x}_j^T(\mathbf{y} - \mathbf{X}\beta) = \lambda \cdot sign(\beta_j), \forall j \in \mathcal{B}\]</p></li>
<li><p>They are identical only if \(sign(\beta_j)\) matches the sign of the lefthand side. In \(\mathcal{A}\), we allow for the \(\beta_j\), where \(sign(\beta_j) \neq sign(\mathbf{x}_j^T(\mathbf{y} - \mathbf{X}\beta))\), while this is forbidden in \(\mathcal{B}\). Thus, once a coefficent hits zero, we drop it.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-43" style="background:;">
  <hgroup>
    <h2>LAR</h2>
  </hgroup>
  <article>
    <h3>Some heuristic analysis</h3>

<ul>
<li> For LAR, we have 
\[|\mathbf{x}_j^T(\mathbf{y} - \mathbf{X}\beta)| \leq \gamma, \forall \mathbf{x}_j \not\in \mathcal{A}\]</li>
<li>According to the stationary conditions, for LASSO, we have
\[
|\mathbf{x}_j^T(\mathbf{y} - \mathbf{X}\beta)| \leq \lambda, \forall \mathbf{x}_j \not\in \mathcal{B}
\]</li>
<li>They match for variables with zero coefficients too.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-44" style="background:;">
  <hgroup>
    <h2>LASSO</h2>
  </hgroup>
  <article>
    <ul>
<li>Introduction to Dimension Reduction</li>
<li>Linear Regression and Least Squares (Review)</li>
<li><b>Shrinkage Method</b>

<ul>
<li>Ridge Regression</li>
<li><b>Lasso</b>

<ul>
<li>Formulations</li>
<li>Comparisons with ridge regression and subset selection</li>
<li>Quadratic Programming</li>
<li>Least Angle Regression</li>
<li><b>Viewed as approximation for \(l_0\)-regularization</b></li>
</ul></li>
</ul></li>
<li>Beyond Lasso</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-45" style="background:;">
  <hgroup>
    <h2>Viewed as approximation for \(l_0\)-regularization</h2>
  </hgroup>
  <article>
    <h3>Pure variable selection</h3>

<p>\[
\begin{equation}
\hat{\beta}^{ridge}= argmin_{\beta}\sum_{i=1}^N(y_i - \beta_0 - \sum_{j=1}^p\mathbf{x_{ij}}\beta_j)^2, s.t. \#nonzero \beta_j \leq t
\end{equation}
\]</p>

<p>Actually \(\#nonzero \beta_j = \|\beta\|_0\), where</p>

<p>\[\|\beta\|_0 = lim_{q \to 0}(\sum_{j = 1}^p|\beta_j|^q)^{\frac{1}{q}} = card(\{\beta_j|\beta_j \neq 0\})\]</p>

<p><center><img src="assets/img/zeronorm.png" alt="zeronorm" title="zeronorm"></center></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-46" style="background:;">
  <hgroup>
    <h2>Viewed as approximation for \(l_0\)-regularization</h2>
  </hgroup>
  <article>
    <h3>Problem</h3>

<p>\(l_0\)-norm is not convex, which makes it very hard to optimize.</p>

<h3>Solutions</h3>

<ul>
<li><b>LASSO</b>: Approximated objective function (\(l_1\)-norm), with exact optimization</li>
<li><b>Subset selection</b>: Exact objective function, with approximated optimization (greedy strategy)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-47" style="background:;">
  <hgroup>
    <h2>Beyond LASSO</h2>
  </hgroup>
  <article>
    <ul>
<li>Introduction to Dimension Reduction</li>
<li>Linear Regression and Least Squares (Review)</li>
<li>Shrinkage Method</li>
<li><b>Beyond LASSO</b>

<ul>
<li><b>Elastic-Net</b></li>
<li><b>Fused Lasso</b></li>
<li><b>Group Lasso</b><br></li>
<li><b>\(l_1-lp\) norm</b></li>
<li><b>Graph-guided Lasso</b></li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-48" style="background:;">
  <hgroup>
    <h2>Beyond LASSO</h2>
  </hgroup>
  <article>
    <h3>Elastic Net</h3>

  

<div class='left' style='float:left;width:41%'>
 <ul>
<li><b>Formualtion</b>
\[\lambda \sum_{j = 1}^p (\alpha \beta_j^2 + (1-\alpha)|\beta_j|)\]
which is a compromise between ridge regression and LASSO.</li>
</ul>


</div>    
<div class='right' style='float:right;width:55%'>
 <p><center><img src="assets/img/enet.png" alt="enet" title="enet"></center></p>


</div>
<div style='float:left;width:100%;' class='centered'>
  <ul>
<li><b>Advantages</b>

<ul>
<li>The elastic-net selects variables like the lasso, and shrinks together the coefficients of correlated predictors like ridge.</li>
<li>It also has considerable computational advantages over the \(l_q\) penalties. (detailed in Section 18.4 in Elements of Statistical Learning)</li>
</ul></li>
</ul>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-49" style="background:;">
  <hgroup>
    <h2>Beyond LASSO</h2>
  </hgroup>
  <article>
    <h3>Fused Lasso</h3>

<ul>
<li><b>Intuition</b>

<ul>
<li>Fused lasso is a generalization that is designed for problems with features that can be ordered in some meaningful way. </li>
<li>The fused lasso penalizes the \(L_1\)-norm of both the coefﬁcients and their successive differences.</li>
</ul></li>
<li><b>Formulation</b></li>
</ul>

<p>\[\hat{\beta} = argmin_{\beta}\{\|\mathbf{X\beta - y}\|_2^2\}\]
\[s.t. \|\beta\| \leq s_1 and \sum_{j = 2}^p |\beta_j - \beta_{j-1}| \leq s_2\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-50" style="background:;">
  <hgroup>
    <h2>Beyond LASSO</h2>
  </hgroup>
  <article>
    <h3>Fused Lasso</h3>

<p><center><img src="assets/img/nb_fs.png" alt="nb_fs" title="nb_fs"></center></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-51" style="background:;">
  <hgroup>
    <h2>Beyond LASSO</h2>
  </hgroup>
  <article>
    <h3>Group Lasso</h3>

<ul>
<li><b>Intuition</b>

<ul>
<li>Features are divided into \(L\) groups</li>
<li>Features within the same group should share similar coefficients</li>
</ul></li>
<li><b>Example</b>

<ul>
<li>Binary dummy variables from one single discrete variable, e.g. \(stage\_cancer \in \{1,2,3\}\) can be translated into three binary dummy variables \((stage1, stage2, stage3)\) </li>
</ul></li>
<li><b>Formulations</b>
\[obj = \|\mathbf{y} - \sum_{l = 1}^L \mathbf{X}_l \beta_l \|_2^2 + \lambda_1 \sum_{l = 1}^L\|\beta_l\|_2 + \lambda_2 \|\beta\|_1\]</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-52" style="background:;">
  <hgroup>
    <h2>Beyond LASSO</h2>
  </hgroup>
  <article>
    <h3>\(l_1\)-\(l_p\) penalization</h3>

<ul>
<li><b>Applies to multi-task learning</b>, where the goal is to estimate predictive models for several related tasks. </li>
<li><b>Examples</b>

<ul>
<li><b>Example 1</b>: recognize speech of different speakers, or handwriting of different writers, </li>
<li><b>Example 2</b>: learn to control a robot for grasping different objects or drive in different landscapes, etc. </li>
</ul></li>
<li><b>Assumptions about the tasks</b>

<ul>
<li>sufficiently <i>different</i> that learning a specific model for each task results in improved performance</li>
<li><i>similar</i> enough that they share some common underlying representation that should make simul- taneous learning beneficial. </li>
<li>In particular, we focus on the scenario where the different tasks share a subset of relevant features to be selected from a large common space of features.</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-53" style="background:;">
  <hgroup>
    <h2>Beyond LASSO</h2>
  </hgroup>
  <article>
    <h3>\(l_1\)-\(l_p\) penalization</h3>

<ul>
<li>Formulation

<ul>
<li>\(\mathbf{X}_l\): \(N \times p\) input matrix for task \(l\) and \(L\) is the total number of tasks</li>
<li>\(\beta\): \(p \times L\) coefficient matrix</li>
<li>\(\mathbf{y}\): \(N \times L\) output matrix</li>
<li>objective function
\[obj = \sum_{l= 1}^L loss(\beta_{:l}, \mathbf{X}_l, \mathbf{y}_{:l}) + \lambda \sum_{j = 1}^p \|\beta_{:j}\|_2\]
where \(loss()\) is some loss function and \(\sum_{j = 1}^p \|\beta_{:j}\|_2\) is equavalent to get the \(l_1\) norm of vector \((\|\beta_{:1}\|_2, \|\beta_{:2}\|_2, ..., \|\beta_{:p}\|_2)\).</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-54" style="background:;">
  <hgroup>
    <h2>Beyond LASSO</h2>
  </hgroup>
  <article>
    <h3>\(l_1-l_p\) penalization -Coefficient matrix</h3>

<p><center><img src="assets/img/l1lp.png" alt="l1lp" title="l1lp"></center></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-55" style="background:;">
  <hgroup>
    <h2>Beyond LASSO</h2>
  </hgroup>
  <article>
    <h3>\(l_1-l_p\) penalization -Norm ball</h3>

<p><center><img src="assets/img/normball.png" alt="normball" title="normball"></center></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-56" style="background:;">
  <hgroup>
    <h2>Beyond LASSO</h2>
  </hgroup>
  <article>
    <h3>Graph-Guided LASSO</h3>

<ul>
<li><b>Example</b>
<center><img src="assets/img/gflasso.png" alt="gflasso" title="gflasso"></center></li>
<li><b>Formulation</b>
Graph-Guided Lasso applies to multi-task settings
\[obj = \sum_{l= 1}^L loss(\beta_{:l}, \mathbf{X}_l, \mathbf{y}_{:l}) + \lambda \|\beta\|_1+\gamma \sum_{e=(a,b)\in E}^p \tau(r_{ab}) \sum_{j = 1}^p |\beta_{ja} - sign(r_{a,b})\beta_{jb}|\]
where \(r_{a,b} \in \mathbb{R}\) denotes the weight of the edge and \(\tau(r)\) can be any positive monotonically increasing function of \(|r|\), e.g. \(\tau(r) = |r|\).</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-57" style="background:;">
  <hgroup>
    <h2>Beyond LASSO</h2>
  </hgroup>
  <article>
    <h3>Graph-Guided LASSO</h3>

<p><center><img src="assets/img/gflasso_re.png" alt="gflasso_re" title="gflasso_re"></center></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-58">
<hgroup>
  <h2>Sparse Models</h2>
</hgroup>
<article class = 'flexbox vcenter'>
<h3>Thank You!</h3>

</article>
<!-- Presenter Notes -->
</slide>
    <slide class="backdrop"></slide>
  </slides>

  <!--[if IE]>
    <script 
      src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js">  
    </script>
    <script>CFInstall.check({mode: 'overlay'});</script>
  <![endif]-->
</body>
<!-- Grab CDN jQuery, fall back to local if offline -->
<script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>
<script>window.jQuery || document.write('<script src="libraries/widgets/quiz/js/jquery-1.7.min.js"><\/script>')</script>
<!-- Load Javascripts for Widgets -->
<!-- MathJax: Fall back to local if CDN offline but local image fonts are not supported (saves >100MB) -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<!-- <script src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script> -->
<script>window.MathJax || document.write('<script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS":{imageFont:null}});<\/script><script src="libraries/widgets/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"><\/script>')
</script>
<!-- LOAD HIGHLIGHTER JS FILES -->
<script src="libraries/highlighters/highlight.js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<!-- DONE LOADING HIGHLIGHTER JS FILES -->
</html>